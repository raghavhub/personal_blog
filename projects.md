---
layout: default
---
## Research

## IQVIA

At IQVIA, I am developing Machine Learning models that can assist doctors with early prognosis of medical conditions in patients.
I am particularly interested in developing vision-based deep learning models that can utilize retinal scans for prognosis.

## Arizona State University

At ASU, I was exploring the problem of cross-modal representation for vision-language tasks such as Visual Question Answering/Image Captioning. I was working on developing attention-based architectures that is better suited for modeling the shared space between vision and language.

[Master's Thesis](https://keep.lib.asu.edu/items/158120)

## Internship - Signify(Philips Lighting)

I spent my summer'20 at Signify research facility located at Cambridge, MA. 
I worked with the smart-lightning R&D team to apply ML in understanding customer behavior in the smart-lighting sector.

## Academia

### Teaching Assistant - CSE 569 - Fundamentals of Statistical Learning

I served as a TA for a graduate course on Machine Learning instructed by Dr. Hemanth Venkateshwara during Fall 2019 at ASU.
The course introduced the students to the concepts in pattern recognition, Bayesian decision theory, parameter estimation, discriminant analysis, and neural networks.

## Other Projects

### Sentiment Analysis - Stock Prediction from News

We studied the impact of digital news on the stock market. We extracted the news articles related to Apple and Amazon from the NASDAQ website. We used the stock variations to identify the sentiment and used them to label the articles as positive or negative. We fine-tuned a BERT classifier using the labeled news dataset and tested it on news samples. We concluded that stock fluctuations at any given time are correlated with market sentiment reflected by the news articles published in the last 4 hours.

[Project link](https://github.com/raghavhub/stock_prediction)


### Adaptive Sampling using KLD - Monte Carlo

This was a Proof-of-concept implementation based on the paper published by Dieter Fox et al. The paper introduced a new adaptive sampling technique based on KL divergence. We implemented the algorithm presented in the paper and demonstrated its ability to score better in a game of Ghostbusters.

[Project link](https://github.com/raghavhub/mc_sampling)

### Gradient Optimization Techniques

We evaluated the performance of the gradient descent optimization techniques such as Polyakâ€™s classical momentum, RMSprop, and Adam Optimization. 
We evaluated the performance of the techniques on a fully connected Neural Network using metrics for accuracy, stability, and training time. The model was trained using 6000 training samples and 1000 test samples from FashionMNIST. 

[Project link](https://github.com/raghavhub/gradient_descent)

[back](./)

