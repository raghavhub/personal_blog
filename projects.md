---
layout: default
---
## Research

### Visual-Language Representation using Cross-Modal Attention

I am currently exploring the problem of cross-modal representation for vision-language tasks such as Visual Question Answering/Image Captioning. There is growing interest in adapting BERT for handling such tasks. I am working on developing an attention-based architecture that can be better suited for modeling the shared space linking vision and language.

## Internship - Signify(Philips Lighting)

I was fortunate to spend my summer at Signify research facility located at Cambridge, MA. 
I worked with the smart-lightning R&D team to apply ML in understanding customer behavior in the smart-lighting sector.

## Academia

### Teaching Assistant - CSE 569 - Fundamentals of Statistical Learning

I served as a TA for a graduate course on Machine Learning instructed by Dr.Hemanth Venkateshwara during Fall 2019 at ASU.
The course introduces the students to the concepts in pattern recognition, Bayesian decision theory, parameter estimation, discriminant analysis, and neural networks.

## Other Projects

### Sentiment Analysis - Stock Prediction from News

We studied the impact of digital news on the stock market. We extracted the news articles related to Apple and Amazon from the NASDAQ website. We used the stock variations to identify the sentiment and used them to label the articles as positive or negative. We fine-tuned a BERT classifier using the labeled news dataset and tested it on news samples. We concluded that stock fluctuations at any given time are correlated with market sentiment reflected by the news articles published in the last 4 hours.

[Project link](https://github.com/raghavhub/stock_prediction)


### Adaptive Sampling using KLD - Monte Carlo

This was a Proof-of-concept implementation based on the paper published by Dieter Fox et al. The paper introduced a new adaptive sampling technique based on KL divergence. We implemented the algorithm presented in the paper and demonstrated its ability to score better in a game of Ghostbusters.

[Project link](https://github.com/raghavhub/mc_sampling)

### Gradient Optimization Techniques

We evaluated the performance of the gradient descent optimization techniques such as Polyakâ€™s classical momentum, RMSprop, and Adam Optimization. 
We evaluated the performance of the techniques on a fully connected Neural Network using metrics for accuracy, stability, and training time. The model was trained using 6000 training samples and 1000 test samples from FashionMNIST. 

[Project link](https://github.com/raghavhub/gradient_descent)

[back](./)

